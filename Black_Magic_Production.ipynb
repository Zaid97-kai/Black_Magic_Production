{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nLlmm9JojEKm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALCSV = 'DATASET_MED/Only_labels/Only_labels_val.csv'\n",
    "TRAINCSV  = 'DATASET_MED/Only_labels/Only_labels_train.csv'\n",
    "LABELS = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
    "          'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule',\n",
    "          'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XDataset(Dataset):\n",
    "    def __init__(self, anno_path, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.imgs = []\n",
    "        self.annos = []\n",
    "        print('loading', anno_path)\n",
    "        \n",
    "        with open(anno_path) as fp:\n",
    "            for i in fp:\n",
    "                if i.split(',')[1].replace('\\n','').split('|')[0] == 'No Finding':\n",
    "                    self.imgs.append(anno_path.replace(anno_path.split('/')[-1],'') + 'Images/' + i.split(',')[0])\n",
    "                    self.annos.append([])\n",
    "                else:\n",
    "                    self.imgs.append(anno_path.replace(anno_path.split('/')[-1],'') + 'Images/' + i.split(',')[0])\n",
    "                    self.annos.append(i.split(',')[1].replace('\\n','').split('|'))\n",
    "                    \n",
    "        self.classes = LABELS\n",
    "        \n",
    "        for item_id in range(len(self.annos)):\n",
    "            item = self.annos[item_id]\n",
    "            vector = [cls in item for cls in self.classes]\n",
    "            self.annos[item_id] = np.array(vector, dtype=float)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        anno = self.annos[item]\n",
    "        img_path = os.path.join(self.imgs[item])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, anno\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = XDataset(os.path.join(VALCSV), None)\n",
    "dataset_train = XDataset(os.path.join(TRAINCSV), None)\n",
    "\n",
    "def show_sample(img, binary_img_labels):\n",
    "    img_labels = np.array(dataset_val.classes)[np.argwhere(binary_img_labels > 0)[:, 0]]\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"{}\".format(', '.join(img_labels)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for sample_id in range(5):\n",
    "    show_sample(*dataset_val[sample_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=resnet.fc.in_features, out_features=n_classes)\n",
    "        )\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro', zero_division=True),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro', zero_division=True),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro', zero_division=True),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro', zero_division=True),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro', zero_division=True),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro', zero_division=True),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples', zero_division=True),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples', zero_division=True),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples', zero_division=True),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "lr = 1e-4 # 1e-3\n",
    "batch_size = 8\n",
    "save_freq = 1 \n",
    "test_freq = 800 \n",
    "max_epoch_number = 35 \n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "save_path = 'chekpoints/'\n",
    "logdir = 'logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint_save(model, save_path, epoch):\n",
    "    f = os.path.join(save_path, 'checkpoint-{:06d}.pth'.format(epoch))\n",
    "    if 'module' in dir(model):\n",
    "        torch.save(model.module.state_dict(), f)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), f)\n",
    "    print('saved checkpoint:', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = transforms.Compose([\n",
    "    #transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotations = os.path.join(VALCSV)\n",
    "train_annotations = os.path.join(TRAINCSV)\n",
    "test_dataset = XDataset(test_annotations, val_transform)\n",
    "train_dataset = XDataset(train_annotations, train_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True,\n",
    "                              drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "num_train_batches = int(np.ceil(len(train_dataset) / batch_size))\n",
    "\n",
    "# model\n",
    "model = Resnet18(len(train_dataset.classes))\n",
    "model.train()\n",
    "model = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "logger = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "epoch = 0\n",
    "iteration = 0\n",
    "while True:\n",
    "    batch_losses = []\n",
    "    for imgs, targets in train_dataloader:\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_result = model(imgs)\n",
    "        loss = criterion(model_result, targets.type(torch.float))\n",
    "\n",
    "        batch_loss_value = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        logger.add_scalar('train_loss', batch_loss_value, iteration)\n",
    "        batch_losses.append(batch_loss_value)\n",
    "        with torch.no_grad():\n",
    "            result = calculate_metrics(model_result.cpu().numpy(), targets.cpu().numpy())\n",
    "            for metric in result:\n",
    "                logger.add_scalar('train/' + metric, result[metric], iteration)\n",
    "\n",
    "        if iteration % test_freq == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                model_result = []\n",
    "                targets = []\n",
    "                for imgs, batch_targets in test_dataloader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    model_batch_result = model(imgs)\n",
    "                    model_result.extend(model_batch_result.cpu().numpy())\n",
    "                    targets.extend(batch_targets.cpu().numpy())\n",
    "\n",
    "            result = calculate_metrics(np.array(model_result), np.array(targets))\n",
    "            for metric in result:\n",
    "                logger.add_scalar('test/' + metric, result[metric], iteration)\n",
    "            print(\"epoch:{:2d} iter:{:3d} test: \"\n",
    "                  \"micro f1: {:.3f} \"\n",
    "                  \"macro f1: {:.3f} \"\n",
    "                  \"samples f1: {:.3f}\".format(epoch, iteration,\n",
    "                                              result['micro/f1'],\n",
    "                                              result['macro/f1'],\n",
    "                                              result['samples/f1']))\n",
    "\n",
    "            model.train()\n",
    "        iteration += 1\n",
    "\n",
    "    loss_value = np.mean(batch_losses)\n",
    "    print(\"epoch:{:2d} iter:{:3d} train: loss:{:.3f}\".format(epoch, iteration, loss_value))\n",
    "    if epoch % save_freq == 0:\n",
    "        checkpoint_save(model, save_path, epoch)\n",
    "    epoch += 1\n",
    "    if max_epoch_number < epoch:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "",
    "kind": "private"
   },
   "name": "",
   "private_outputs": true,
   "provenance": [
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
